<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2020/04/07/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<a id="more"></a>

<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>MapReduce编程实践1</title>
    <url>/2020/04/08/Large-scaleDistributedSystem01/</url>
    <content><![CDATA[<h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><p>MapReduce是一种新的分布式并行计算框架，能够在HDFS和Yarn的支持下完成海量大数据计算与处理的任务。最早起源于Google的GFS系统，是Google为了处理PB级别的网页数据而设计的。<br>MapReduce的优势在于可以对海量数据进行并行计算，并且能够高效的部署在廉价的计算机集群上，因而受到广泛使用。它的基本思想是分而治之。</p>
<p>MapReduce借鉴了函数式程序设计语言Lisp中的思想，定义了如下的Map和Reduce两个抽象的编程接口，由用户去编程实现:</p>
<ul>
<li>Map: (k1; v1) -&gt; [(k2; v2)]</li>
<li>Reduce: : (k2; [v2]) -&gt; [(k3; v3)]</li>
</ul>
<p>各个map函数对所划分的数据并行处理，从不同的输入数据产生不同的中间结果输出。<br>各个reduce也各自并行计算，各自负责处理不同的中间结果数据集合。</p>
<a id="more"></a>

<h3 id="本次任务"><a href="#本次任务" class="headerlink" title="本次任务"></a>本次任务</h3><p>编写MR程序（java或python）对sample.txt中有关文件信息进行处理，任务如下：<br>&nbsp;&nbsp;&nbsp;&nbsp;1.统计其中各类文件的数量（按文件名后缀区分类型）<br>&nbsp;&nbsp;&nbsp;&nbsp;2.按文件的字节数大小降序排序输出文件名</p>
<p>sample.txt 包含85行建筑设计中的各类文件信息，数据片段和格式说明如下:</p>
<table>
    <tr>
        <th rowspan="3">sample.txt</th>
        <th>日期</th>
        <th>时间</th>
        <th>字节数</th>
        <th>文件名（后缀为dwg）</th>
    </tr>
    <tr>
        <td>2014/10/21</td>
        <td>07:30</td>
        <td>3,395,145</td>
        <td>02-01一层平面图.dwg</td>
    </tr>
    <tr>
        <td>2014/10/21</td>
        <td>07:29</td>
        <td>924,099</td>
        <td>02-02二层平面图.dwg</td>
</tr></table>

<h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><p>环境配置</p>
<ul>
<li>阿里云ECS Ubuntu 16.04</li>
<li>java: open jdk-7</li>
<li>hadoop: 2.9.2</li>
<li>python: 3.5<ul>
<li>mrjob 0.7.1</li>
</ul>
</li>
</ul>
<h4 id="统计各类文件的数量"><a href="#统计各类文件的数量" class="headerlink" title="统计各类文件的数量"></a>统计各类文件的数量</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mrjob.job <span class="keyword">import</span> MRJob</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">pattern1 = <span class="string">r"[0-9]*,*[0-9]+,[0-9]&#123;3&#125;"</span> <span class="comment"># number of bytes</span></span><br><span class="line">pattern2 = <span class="string">r"\.[a-z]+"</span> <span class="comment"># file type</span></span><br><span class="line">pattern3 = <span class="string">r" [^ ]+\.[a-z]+$"</span> <span class="comment"># file name</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">str2int</span><span class="params">(input)</span>:</span></span><br><span class="line">    a = <span class="string">""</span>.join(input.split(<span class="string">','</span>))</span><br><span class="line">    <span class="keyword">return</span> int(a)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CountWorker</span><span class="params">(MRJob)</span>:</span></span><br><span class="line">    <span class="string">""" Count the number of each type of files. """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mapper</span><span class="params">(self, file_name, line)</span>:</span></span><br><span class="line">        <span class="string">""" Input Context: Date, Time, #Bytes, Name """</span></span><br><span class="line">        file_type = re.search(pattern2, line).group()[<span class="number">1</span>:]</span><br><span class="line">        <span class="keyword">yield</span> file_type, <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reducer</span><span class="params">(self, key, values)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> key, sum(values)</span><br></pre></td></tr></table></figure>

<h4 id="按文件字节数降序输出文件名"><a href="#按文件字节数降序输出文件名" class="headerlink" title="按文件字节数降序输出文件名"></a>按文件字节数降序输出文件名</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RankWorker</span><span class="params">(MRJob)</span>:</span></span><br><span class="line">    <span class="string">""" Ranking in the descending order of #Bytes. """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mapper</span><span class="params">(self, file_name, line)</span>:</span></span><br><span class="line">        <span class="string">""" Input Context: Date, Time, #Bytes, Name """</span></span><br><span class="line">        num_bytes = str2int(re.search(pattern1, line).group())</span><br><span class="line">        Name = re.search(pattern3, line).group()[<span class="number">1</span>:]</span><br><span class="line">        <span class="keyword">yield</span> <span class="number">1</span>, (num_bytes, Name)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reducer</span><span class="params">(self, key, values)</span>:</span></span><br><span class="line">        values = sorted(values)</span><br><span class="line">        <span class="keyword">for</span> num_bytes, Name <span class="keyword">in</span> values[::<span class="number">-1</span>]:</span><br><span class="line">            <span class="keyword">yield</span> num_bytes, Name</span><br></pre></td></tr></table></figure>

<h4 id="输出中文乱码问题"><a href="#输出中文乱码问题" class="headerlink" title="输出中文乱码问题"></a>输出中文乱码问题</h4><p>注意，这里由于一开始的时候没有对sample.txt文件进行转码，而且由于MRjob的输出设定，返回的字符串并不是utf-8编码的。因此需要手动进行一下解码（可能需要多次尝试）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deal_with_output</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    lines = []</span><br><span class="line">    <span class="keyword">with</span> open(file_path, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">            line = f.readline()</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> line: <span class="keyword">break</span></span><br><span class="line">            tmp = line.strip().split(<span class="string">"\t"</span>)</span><br><span class="line">            </span><br><span class="line">            num_bytes = int(tmp[<span class="number">0</span>])</span><br><span class="line">            name = <span class="string">""</span>.join([tmp[<span class="number">1</span>][<span class="number">1</span>:<span class="number">-1</span>]])</span><br><span class="line">            </span><br><span class="line">            print(num_bytes, name.encode(<span class="string">"iso-8859-1"</span>).decode(<span class="string">"gb18030"</span>))</span><br><span class="line">            <span class="comment"># 这里还是有bug，建议根据情况自己试一下</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>MapReduce编程实践2：倒排文档索引</title>
    <url>/2020/04/08/Large-scaleDistributedSystem02/</url>
    <content><![CDATA[<h3 id="1-具有payload的倒排文档索引"><a href="#1-具有payload的倒排文档索引" class="headerlink" title="1.具有payload的倒排文档索引"></a>1.具有payload的倒排文档索引</h3><p>Inverted Index(倒排索引)是几乎所有全文检索搜索引擎都在使用的一种数据索引技术。<br>采用倒排索引，对于给定的查询词(term)，能快速或取包含有该term的文档列表(the list of documents)。</p>
<a id="more"></a>

<p>简单的倒排文档索引示意图:</p>
<p><img src="/2020/04/08/Large-scaleDistributedSystem02/invdoc1.png" alt="简单的倒排文档索引示意图"></p>
<p>如果考虑单词在每个文档中出现的词频、位置、对应Web文档的URL等诸多属性，则简单的倒排算法就需要进一步的改进。我们把这些词频、位置等诸多属性称为有效负载（Payload）。</p>
<img src="/2020/04/08/Large-scaleDistributedSystem02/invdoc2.png" class>
<p>一个倒排索引由大量的postings list构成，每个posting list与一个term相关联。一个posting 包含一个document id和一个payload。payload上载有term在document中出现情况相关的信息（例如：出现位置，词频等等）。</p>
<p>前述提到，MapReduce的核心是Map和Reduce两个函数。Map用于生成会被Reduce使用的中间结果。由于MR系统是运行在分布式集群上的，所以中间结果会被Partition和Combine(可选的)。在Parition过程中，系统自动按照map的输出键进行排序，因此，进入Reduce节点的(key, {value})对将保证是按照key进行排序的，而{value}则不保证是排好序的。</p>
<p>那如果想要对value进行排序，应该怎么办呢？</p>
<ul>
<li>一种办法是在Reduce任务中，对{value}表中的各个value进行排序。但当{value}列表数据量巨大、无法在本地内存中进行排序时，将出现问题。</li>
<li>更好的办法是利用MapReduce过程中的Partition过程会对Map输出key进行排序的效应，在分发过程中就完成对value的排序。想要做到这一点，就需要将value中需要排序的部分加入到key中，使key成为复合键。 </li>
</ul>
<p>举例来说，本来Map任务输出的键值对是（word, (doc_titile, word_frequency)）。现在为了实现对word_frequency的排序，我们将Map的输出重置为（(word, word_frequency), doc_titile）。</p>
<p>但这样会带来一个问题：我们仍然想要将同一个word对应的key-value pair放到同一个reduce任务下，但是现在的key是一个复合键。为此，需要实现一个新的Partitioner：从（word, word_frequency）中取出word，以word作为key进行分区。</p>
<p>了解到这些以后，就可以进行MR倒排文档生成了。</p>
<h3 id="2-本次任务"><a href="#2-本次任务" class="headerlink" title="2.本次任务"></a>2.本次任务</h3><ul>
<li>倒排文档生成函数：带有payload的倒排文档索引</li>
<li>文档查询函数：以contenttile为文本名。实现根据查询关键词，返回相关文档名（序列）</li>
<li>提交查询返回结果的截屏</li>
</ul>
<p>数据样本下载地址：<a href="https://pan.baidu.com/s/1UZHOMw_uhLMBCLX9uBBAQA" target="_blank" rel="noopener">密码 : rdx8</a></p>
<h3 id="3-代码示例"><a href="#3-代码示例" class="headerlink" title="3.代码示例"></a>3.代码示例</h3><p>环境配置</p>
<ul>
<li>阿里云ECS Ubuntu 16.04</li>
<li>java: open jdk-7</li>
<li>hadoop: 2.9.2</li>
<li>python: 3.5</li>
</ul>
<h4 id="3-1-解决中文乱码问题"><a href="#3-1-解决中文乱码问题" class="headerlink" title="3.1 解决中文乱码问题"></a>3.1 解决中文乱码问题</h4><p>由于下载的文件不是utf-8编码，而是gbk编码的。为了后续编程方便和一致性(jieba的官方文档说如果输入的字符串是gbk编码可能会出现未知错误)，建议将文件先转码为utf-8编码。具体操作是：首先在MacOS文本编辑器的偏好设置里，将文件的打开编码设置为gbk，保存编码设置为utf-8，然后打开下载的文件，另存为…即可。然后要记得把文本编辑器的偏好设置改回来（即打开/关闭编码都最好是utf-8）。</p>
<p>然后，将下载的文件scp到已经配好环境的ECS服务器上。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp local_file_path username@server:host_file_path</span><br></pre></td></tr></table></figure>
<p>这里建议不管是不是使用云服务器，都检查一下当前使用终端的字符集是否支持中文。如果不支持，在服务器上还是会出现中文乱码问题，可以参照这个链接：<a href="https://blog.csdn.net/weixin_34236497/article/details/94046218" target="_blank" rel="noopener">ubuntu16.04解决文件中文乱码问题</a>。</p>
<h4 id="3-2-mapper-py"><a href="#3-2-mapper-py" class="headerlink" title="3.2 mapper.py"></a>3.2 mapper.py</h4><p>与上一篇mapreduce实践不同，这次因为要实现payload，所以选用了hadoop streaming来编程。相比mrjob，hadoop streaming更为灵活，用户能够自定义更多特性，而且输入输出都是字符串的特性也比较简单好操作。具体的一些工作原理可以参考官方文档：<a href="http://hadoop.apache.org/docs/r1.0.4/cn/streaming.html" target="_blank" rel="noopener">Hadoop Streaming</a>。</p>
<p>这里简单介绍一下，hadoop streaming支持多种语言进行编程，包括java 和python。它只规定了mapper和reducer都必须是可执行文件。这次实践我们采用hadoop streaming + python。</p>
<p>mapper.py从stdin中读入文件，它要做的事是从输入的字符串中：</p>
<ol>
<li>识别出文档内容和文档名称</li>
<li>对文档内容进行分词（需要安装python第三方库jieba）</li>
<li>返回(单词，所在文档，词频)这样的key-value pair</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">pattern1 = <span class="string">r"&lt;[a-zA-Z]*&gt;"</span>  <span class="comment"># &lt;&gt;</span></span><br><span class="line">pattern2 = <span class="string">r"&gt;[^a-z]*"</span>  <span class="comment"># &gt;&lt;</span></span><br><span class="line">pattern3 = <span class="string">r"\".+?\""</span></span><br><span class="line"></span><br><span class="line">contents = []</span><br><span class="line">titles = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">    result = re.search(pattern1, line)</span><br><span class="line">    <span class="keyword">if</span> result <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        typo = result.group()[<span class="number">1</span>:<span class="number">-1</span>]</span><br><span class="line">        _text = re.search(pattern2, line).group()[<span class="number">1</span>:<span class="number">-2</span>] <span class="comment"># omit "\n" and "&gt;"</span></span><br><span class="line">        <span class="keyword">if</span> typo == <span class="string">"content"</span>:</span><br><span class="line">            contents.append(_text)</span><br><span class="line">        <span class="keyword">elif</span> typo == <span class="string">"contenttitle"</span>:</span><br><span class="line">            _text = _text[:<span class="number">-1</span>] <span class="keyword">if</span> (_text != <span class="string">""</span> <span class="keyword">and</span> _text[<span class="number">-1</span>] == <span class="string">"（"</span>) <span class="keyword">else</span> _text</span><br><span class="line">            titles.append(_text)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(titles)):</span><br><span class="line">    title, doc = titles[i], contents[i]</span><br><span class="line">    seg_list = list(jieba.cut(doc, cut_all=<span class="literal">True</span>)) <span class="comment"># precise mode</span></span><br><span class="line">    counter = Counter(seg_list)</span><br><span class="line">    <span class="keyword">for</span> pos, word <span class="keyword">in</span> enumerate(set(seg_list)):</span><br><span class="line">        print(<span class="string">"&#123;&#125;@&#123;&#125;@&#123;&#125;"</span>.format(word, counter[word], title))</span><br></pre></td></tr></table></figure>

<h4 id="3-3-reducer-py"><a href="#3-3-reducer-py" class="headerlink" title="3.3 reducer.py"></a>3.3 reducer.py</h4><p>reducer要做的事情，是从stdin中读入mapper的输出，按照（单词，所有包含该单词的文档）形式组织好数据并输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">out_dict = defaultdict(list)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">    key, value = line.split(<span class="string">"\t"</span>)</span><br><span class="line">    word, count = key.split(<span class="string">"@"</span>)</span><br><span class="line">    out_dict[word].append(value[:<span class="number">-1</span>]+<span class="string">"@"</span>+count)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word, value_list <span class="keyword">in</span> out_dict.items():</span><br><span class="line">    titles = <span class="string">"&lt;SEP&gt;"</span>.join(value_list)</span><br><span class="line">    print(<span class="string">"&#123;&#125;\t&#123;&#125;"</span>.format(word, titles))</span><br></pre></td></tr></table></figure>

<h4 id="3-4-run-sh"><a href="#3-4-run-sh" class="headerlink" title="3.4 run.sh"></a>3.4 run.sh</h4><p>在前文中我们有提到，为了将同一个word对应的key-value pair放到同一个reduce任务下，需要实现一个新的Partitioner：从（word, word_frequency）中取出word，以word作为key进行分区。在Hadoop Streaming中这个具体应该怎么实现呢？</p>
<p>很简单。Hadoop有一个工具类org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner， 它在应用程序中很有用。Map/reduce框架用这个类切分map的输出， 切分是基于key值的前缀，而不是整个key。举例来说：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="variable">$HADOOP_HOME</span>/bin/hadoop  jar <span class="variable">$HADOOP_HOME</span>/hadoop-streaming.jar \</span><br><span class="line">    -input myInputDirs \</span><br><span class="line">    -output myOutputDir \</span><br><span class="line">    -mapper org.apache.hadoop.mapred.lib.IdentityMapper \</span><br><span class="line">    -reducer org.apache.hadoop.mapred.lib.IdentityReducer \</span><br><span class="line">    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \</span><br><span class="line">    -jobconf stream.map.output.field.separator=. \</span><br><span class="line">    -jobconf stream.num.map.output.key.fields=4 \</span><br><span class="line">    -jobconf map.output.key.field.separator=. \</span><br><span class="line">    -jobconf num.key.fields.for.partition=2 \</span><br><span class="line">    -jobconf mapred.reduce.tasks=12</span><br></pre></td></tr></table></figure>
<p>其中，Streaming使用-jobconf stream.map.output.field.separator=. 和-jobconf stream.num.map.output.key.fields=4这两个变量来得到mapper的key/value对。</p>
<p>上面的Map/Reduce 作业中map输出的key一般是由“.”分割成的四块。但是因为使用了 -jobconf num.key.fields.for.partition=2 选项，所以Map/Reduce框架使用key的前两块来切分map的输出。其中， -jobconf map.output.key.field.separator=. 指定了这次切分使用的key的分隔符。这样可以保证在所有key/value对中， key值前两个块值相同的所有key被分到一组，分配给一个reducer。</p>
<p>这种高效的方法等价于指定前两块作为主键，后两块作为副键。 主键用于切分块，主键和副键的组合用于排序。</p>
<p>因此，我们只需要在执行程序的时候指明所用的partitioner和分区，就可以实现“按照一部分key的值进行排序”的功能。这里，为了避免每次启动hadoop的时候都要反复手动输入shell脚本，我写了一个.sh文件来保存会在终端进行的输入。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rm ./part-00000 <span class="comment"># remove loacal file</span></span><br><span class="line">hdfs dfs -rm -r YOUR_PATH/output/ <span class="comment"># remove output dir in hdfs, otherwise it will cause en error</span></span><br><span class="line"></span><br><span class="line">hadoop jar <span class="variable">$HADOOP_HOME</span>/share/hadoop/tools/lib/hadoop-streaming-2.9.2.jar \</span><br><span class="line">-input YOUR_PATH/inputs/news_tensite_xml.smarty.dat \</span><br><span class="line">-output YOUR_PATH/output/ \</span><br><span class="line">-mapper <span class="string">"python3 mapper.py"</span> \</span><br><span class="line">-reducer <span class="string">"python3 reducer.py"</span> \</span><br><span class="line">-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \</span><br><span class="line">-jobconf stream.map.output.field.separator=@ \</span><br><span class="line">-jobconf stream.num.map.output.key.fields=2 \</span><br><span class="line">-jobconf map.output.key.field.separator=@ \</span><br><span class="line">-jobconf num.key.fields.for.partition=1 \</span><br><span class="line"></span><br><span class="line">hdfs dfs -get bigdata/output/part-00000 ./ <span class="comment"># copy file from hdfs to local</span></span><br><span class="line">head -100 part-00000 <span class="comment"># look at content</span></span><br></pre></td></tr></table></figure>

<h4 id="3-5-query-py"><a href="#3-5-query-py" class="headerlink" title="3.5 query.py"></a>3.5 query.py</h4><p>文档查询函数：以contenttile为文本名。实现根据查询关键词，返回相关文档名（序列）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">query_dict = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"./part-00000"</span>, <span class="string">"r"</span>, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    lines = f.readlines()</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">        word, rest = line.split(<span class="string">"\t"</span>)</span><br><span class="line">        titles = rest.split(<span class="string">"&lt;SEP&gt;"</span>)</span><br><span class="line">        query_dict[word] = titles</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">query</span><span class="params">(word)</span>:</span></span><br><span class="line">    print(word)</span><br><span class="line">    leng = len(word)</span><br><span class="line">    <span class="keyword">for</span> title <span class="keyword">in</span> query_dict.get(word, [<span class="string">""</span>]):</span><br><span class="line">        print(<span class="string">" "</span> * leng + <span class="string">"\t"</span>, title)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    query(<span class="string">"经济"</span>)</span><br><span class="line">    query(<span class="string">"儿童"</span>)</span><br></pre></td></tr></table></figure>
<h4 id="3-5-Outcome"><a href="#3-5-Outcome" class="headerlink" title="3.5 Outcome"></a>3.5 Outcome</h4><p>query结果：<br><img src="/2020/04/08/Large-scaleDistributedSystem02/invdoc3.png" alt="query_outcome"></p>
<p>这个query结果还是不完善的，可以看到输出的文档名并没有完全按照词频排序（局部有序）。当然，也可以让输出文档名按照tf-idf排序。具体的实现这里就不再赘述。</p>
<h3 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h3><p>在本次实践中，学习了如何避免linux出现中文乱码问题。学会了基础的python和hadoop streaming交互，了解了hadoop streaming的基本使用。进一步加深了对MapReduce框架的理解。</p>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
        <tag>Python</tag>
      </tags>
  </entry>
</search>

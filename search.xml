<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>高级大数据解析01：CUDA编程</title>
    <url>/2020/04/08/AdvancedBigdataAnalysis01/</url>
    <content><![CDATA[<h3 id="Practice01"><a href="#Practice01" class="headerlink" title="Practice01"></a>Practice01</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// kernel routine// </span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">my_first_kernel</span><span class="params">(<span class="keyword">float</span> *x)</span></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> tid = threadIdx.x + blockDim.x*blockIdx.x;</span><br><span class="line">  x[tid] = (<span class="keyword">float</span>) threadIdx.x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// main code //</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span>&#123;</span><br><span class="line">  <span class="keyword">float</span> *h_x, *d_x;</span><br><span class="line">  <span class="keyword">int</span>   nblocks, nthreads, nsize, n; </span><br><span class="line">  </span><br><span class="line">  <span class="comment">// set number of blocks, and threads per block</span></span><br><span class="line">  nblocks  = <span class="number">2</span>;</span><br><span class="line">  nthreads = <span class="number">8</span>;</span><br><span class="line">  nsize    = nblocks*nthreads ;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// allocate memory for array</span></span><br><span class="line">  h_x = (<span class="keyword">float</span> *)<span class="built_in">malloc</span>(nsize*<span class="keyword">sizeof</span>(<span class="keyword">float</span>));</span><br><span class="line">  cudaMalloc((<span class="keyword">void</span> **)&amp;d_x, nsize*<span class="keyword">sizeof</span>(<span class="keyword">float</span>));</span><br><span class="line"></span><br><span class="line">  <span class="comment">// execute kernel</span></span><br><span class="line">  my_first_kernel&lt;&lt;&lt;nblocks,nthreads&gt;&gt;&gt;(d_x);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// copy back results and print them out</span></span><br><span class="line">  cudaMemcpy(h_x,d_x,nsize*<span class="keyword">sizeof</span>(<span class="keyword">float</span>),cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (n=<span class="number">0</span>; n&lt;nsize; n++) <span class="built_in">printf</span>(<span class="string">" n,  x  =  %d  %f \n"</span>,n,h_x[n]);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// free memory </span></span><br><span class="line">  cudaFree(d_x);</span><br><span class="line">  <span class="built_in">free</span>(h_x);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// CUDA exit -- needed to flush printf write buffer</span></span><br><span class="line">  cudaDeviceReset();</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ./prac1</span><br><span class="line">make</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>CUDA</tag>
        <tag>NVIDIA</tag>
        <tag>Cpp</tag>
      </tags>
  </entry>
  <entry>
    <title>MapReduce编程实践2：倒排文档索引</title>
    <url>/2020/04/08/Large-scaleDistributedSystem02/</url>
    <content><![CDATA[<h3 id="1-具有payload的倒排文档索引"><a href="#1-具有payload的倒排文档索引" class="headerlink" title="1.具有payload的倒排文档索引"></a>1.具有payload的倒排文档索引</h3><p>Inverted Index(倒排索引)是几乎所有全文检索搜索引擎都在使用的一种数据索引技术。<br>采用倒排索引，对于给定的查询词(term)，能快速或取包含有该term的文档列表(the list of documents)。</p>
<a id="more"></a>

<p>简单的倒排文档索引示意图:</p>
<p><img src="/2020/04/08/Large-scaleDistributedSystem02/invdoc1.png" alt="简单的倒排文档索引示意图"></p>
<p>如果考虑单词在每个文档中出现的词频、位置、对应Web文档的URL等诸多属性，则简单的倒排算法就需要进一步的改进。我们把这些词频、位置等诸多属性称为有效负载（Payload）。</p>
<img src="/2020/04/08/Large-scaleDistributedSystem02/invdoc2.png" class>
<p>一个倒排索引由大量的postings list构成，每个posting list与一个term相关联。一个posting 包含一个document id和一个payload。payload上载有term在document中出现情况相关的信息（例如：出现位置，词频等等）。</p>
<p>前述提到，MapReduce的核心是Map和Reduce两个函数。Map用于生成会被Reduce使用的中间结果。由于MR系统是运行在分布式集群上的，所以中间结果会被Partition和Combine(可选的)。在Parition过程中，系统自动按照map的输出键进行排序，因此，进入Reduce节点的(key, {value})对将保证是按照key进行排序的，而{value}则不保证是排好序的。</p>
<p>那如果想要对value进行排序，应该怎么办呢？</p>
<ul>
<li>一种办法是在Reduce任务中，对{value}表中的各个value进行排序。但当{value}列表数据量巨大、无法在本地内存中进行排序时，将出现问题。</li>
<li>更好的办法是利用MapReduce过程中的Partition过程会对Map输出key进行排序的效应，在分发过程中就完成对value的排序。想要做到这一点，就需要将value中需要排序的部分加入到key中，使key成为复合键。 </li>
</ul>
<p>举例来说，本来Map任务输出的键值对是（word, (doc_titile, word_frequency)）。现在为了实现对word_frequency的排序，我们将Map的输出重置为（(word, word_frequency), doc_titile）。</p>
<p>但这样会带来一个问题：我们仍然想要将同一个word对应的key-value pair放到同一个reduce任务下，但是现在的key是一个复合键。为此，需要实现一个新的Partitioner：从（word, word_frequency）中取出word，以word作为key进行分区。</p>
<p>了解到这些以后，就可以进行MR倒排文档生成了。</p>
<h3 id="2-本次任务"><a href="#2-本次任务" class="headerlink" title="2.本次任务"></a>2.本次任务</h3><ul>
<li>倒排文档生成函数：带有payload的倒排文档索引</li>
<li>文档查询函数：以contenttile为文本名。实现根据查询关键词，返回相关文档名（序列）</li>
<li>提交查询返回结果的截屏</li>
</ul>
<p>数据样本下载地址：<a href="https://pan.baidu.com/s/1UZHOMw_uhLMBCLX9uBBAQA" target="_blank" rel="noopener">密码 : rdx8</a></p>
<h3 id="3-代码示例"><a href="#3-代码示例" class="headerlink" title="3.代码示例"></a>3.代码示例</h3><p>环境配置</p>
<ul>
<li>阿里云ECS Ubuntu 16.04</li>
<li>java: open jdk-7</li>
<li>hadoop: 2.9.2</li>
<li>python: 3.5</li>
</ul>
<h4 id="3-1-解决中文乱码问题"><a href="#3-1-解决中文乱码问题" class="headerlink" title="3.1 解决中文乱码问题"></a>3.1 解决中文乱码问题</h4><p>由于下载的文件不是utf-8编码，而是gbk编码的。为了后续编程方便和一致性，建议将文件先转码为utf-8编码。具体操作是：首先在MacOS文本编辑器的偏好设置里，将文件的打开编码设置为gbk，保存编码设置为utf-8，然后打开下载的文件，另存为…即可。然后要记得把文本编辑器的偏好设置改回来（即打开/关闭编码都最好是utf-8）。</p>
<p>然后，将下载的文件scp到已经配好环境的ECS服务器上。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp local_file_path username@server:host_file_path</span><br></pre></td></tr></table></figure>
<p>这里建议不管是不是使用云服务器，都检查一下当前使用终端的字符集是否支持中文。如果不支持，在服务器上还是会出现中文乱码问题，可以参照这个链接<a href="https://blog.csdn.net/weixin_34236497/article/details/94046218" target="_blank" rel="noopener">ubuntu16.04解决文件中文乱码问题</a>。</p>
<h4 id="3-2-mapper-py"><a href="#3-2-mapper-py" class="headerlink" title="3.2 mapper.py"></a>3.2 mapper.py</h4><h4 id="3-3-reducer-py"><a href="#3-3-reducer-py" class="headerlink" title="3.3 reducer.py"></a>3.3 reducer.py</h4><h4 id="3-4-run-sh"><a href="#3-4-run-sh" class="headerlink" title="3.4 run.sh"></a>3.4 run.sh</h4><h4 id="3-5-query-py"><a href="#3-5-query-py" class="headerlink" title="3.5 query.py"></a>3.5 query.py</h4><h4 id="3-5-Outcome"><a href="#3-5-Outcome" class="headerlink" title="3.5 Outcome"></a>3.5 Outcome</h4>]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>大规模分布式系统</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/04/07/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<a id="more"></a>

<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>MapReduce编程实践1</title>
    <url>/2020/04/08/Large-scaleDistributedSystem01/</url>
    <content><![CDATA[<h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><p>MapReduce是一种新的分布式并行计算框架，能够在HDFS和Yarn的支持下完成海量大数据计算与处理的任务。最早起源于Google的GFS系统，是Google为了处理PB级别的网页数据而设计的。<br>MapReduce的优势在于可以对海量数据进行并行计算，并且能够高效的部署在廉价的计算机集群上，因而受到广泛使用。它的基本思想是分而治之。</p>
<p>MapReduce借鉴了函数式程序设计语言Lisp中的思想，定义了如下的Map和Reduce两个抽象的编程接口，由用户去编程实现:</p>
<ul>
<li>Map: (k1; v1) -&gt; [(k2; v2)]</li>
<li>Reduce: : (k2; [v2]) -&gt; [(k3; v3)]</li>
</ul>
<p>各个map函数对所划分的数据并行处理，从不同的输入数据产生不同的中间结果输出。<br>各个reduce也各自并行计算，各自负责处理不同的中间结果数据集合。</p>
<a id="more"></a>

<h3 id="本次任务"><a href="#本次任务" class="headerlink" title="本次任务"></a>本次任务</h3><p>编写MR程序（java或python）对sample.txt中有关文件信息进行处理，任务如下：<br>&nbsp;&nbsp;&nbsp;&nbsp;1.统计其中各类文件的数量（按文件名后缀区分类型）<br>&nbsp;&nbsp;&nbsp;&nbsp;2.按文件的字节数大小降序排序输出文件名</p>
<p>sample.txt 包含85行建筑设计中的各类文件信息，数据片段和格式说明如下:</p>
<table>
    <tr>
        <th rowspan="3">sample.txt</th>
        <th>日期</th>
        <th>时间</th>
        <th>字节数</th>
        <th>文件名（后缀为dwg）</th>
    </tr>
    <tr>
        <td>2014/10/21</td>
        <td>07:30</td>
        <td>3,395,145</td>
        <td>02-01一层平面图.dwg</td>
    </tr>
    <tr>
        <td>2014/10/21</td>
        <td>07:29</td>
        <td>924,099</td>
        <td>02-02二层平面图.dwg</td>
</tr></table>

<h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><p>环境配置</p>
<ul>
<li>阿里云ECS Ubuntu 16.04</li>
<li>java: open jdk-7</li>
<li>hadoop: 2.9.2</li>
<li>python: 3.5<ul>
<li>mrjob 0.7.1</li>
</ul>
</li>
</ul>
<h4 id="统计各类文件的数量"><a href="#统计各类文件的数量" class="headerlink" title="统计各类文件的数量"></a>统计各类文件的数量</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mrjob.job <span class="keyword">import</span> MRJob</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">pattern1 = <span class="string">r"[0-9]*,*[0-9]+,[0-9]&#123;3&#125;"</span> <span class="comment"># number of bytes</span></span><br><span class="line">pattern2 = <span class="string">r"\.[a-z]+"</span> <span class="comment"># file type</span></span><br><span class="line">pattern3 = <span class="string">r" [^ ]+\.[a-z]+$"</span> <span class="comment"># file name</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">str2int</span><span class="params">(input)</span>:</span></span><br><span class="line">    a = <span class="string">""</span>.join(input.split(<span class="string">','</span>))</span><br><span class="line">    <span class="keyword">return</span> int(a)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CountWorker</span><span class="params">(MRJob)</span>:</span></span><br><span class="line">    <span class="string">""" Count the number of each type of files. """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mapper</span><span class="params">(self, file_name, line)</span>:</span></span><br><span class="line">        <span class="string">""" Input Context: Date, Time, #Bytes, Name """</span></span><br><span class="line">        file_type = re.search(pattern2, line).group()[<span class="number">1</span>:]</span><br><span class="line">        <span class="keyword">yield</span> file_type, <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reducer</span><span class="params">(self, key, values)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> key, sum(values)</span><br></pre></td></tr></table></figure>

<h4 id="按文件字节数降序输出文件名"><a href="#按文件字节数降序输出文件名" class="headerlink" title="按文件字节数降序输出文件名"></a>按文件字节数降序输出文件名</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RankWorker</span><span class="params">(MRJob)</span>:</span></span><br><span class="line">    <span class="string">""" Ranking in the descending order of #Bytes. """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mapper</span><span class="params">(self, file_name, line)</span>:</span></span><br><span class="line">        <span class="string">""" Input Context: Date, Time, #Bytes, Name """</span></span><br><span class="line">        num_bytes = str2int(re.search(pattern1, line).group())</span><br><span class="line">        Name = re.search(pattern3, line).group()[<span class="number">1</span>:]</span><br><span class="line">        <span class="keyword">yield</span> <span class="number">1</span>, (num_bytes, Name)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reducer</span><span class="params">(self, key, values)</span>:</span></span><br><span class="line">        values = sorted(values)</span><br><span class="line">        <span class="keyword">for</span> num_bytes, Name <span class="keyword">in</span> values[::<span class="number">-1</span>]:</span><br><span class="line">            <span class="keyword">yield</span> num_bytes, Name</span><br></pre></td></tr></table></figure>

<h4 id="输出中文乱码问题"><a href="#输出中文乱码问题" class="headerlink" title="输出中文乱码问题"></a>输出中文乱码问题</h4><p>注意，这里由于一开始的时候没有对sample.txt文件进行转码，而且由于MRjob的输出设定，返回的字符串并不是utf-8编码的。因此需要手动进行一下解码（可能需要多次尝试）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deal_with_output</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    lines = []</span><br><span class="line">    <span class="keyword">with</span> open(file_path, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">            line = f.readline()</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> line: <span class="keyword">break</span></span><br><span class="line">            tmp = line.strip().split(<span class="string">"\t"</span>)</span><br><span class="line">            </span><br><span class="line">            num_bytes = int(tmp[<span class="number">0</span>])</span><br><span class="line">            name = <span class="string">""</span>.join([tmp[<span class="number">1</span>][<span class="number">1</span>:<span class="number">-1</span>]])</span><br><span class="line">            </span><br><span class="line">            print(num_bytes, name.encode(<span class="string">"iso-8859-1"</span>).decode(<span class="string">"gb18030"</span>))</span><br><span class="line">            <span class="comment"># 这里还是有bug，建议根据情况自己试一下</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>大规模分布式系统</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
        <tag>Python</tag>
      </tags>
  </entry>
</search>

<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>高级大数据解析01：CUDA</title>
    <url>/2020/04/09/AdvancedBigdataAnalysis01/</url>
    <content><![CDATA[<h3 id="1-NVIDIA-GPU"><a href="#1-NVIDIA-GPU" class="headerlink" title="1 NVIDIA GPU"></a>1 NVIDIA GPU</h3><p>GPU（Graphics Processing Unit），图形处理器，又称显卡，是一种专门在个人电脑、工作站、游戏机和一些移动设备（如平板电脑、智能手机等）上做图像和图形相关运算工作的微处理器。</p>
<p>目前全球有两大GPU生产商，分别是NVIDIA和ATI(已被AMD收购)。这里主要介绍NVIDIA GPU。</p>
<a id="more"></a>

<h4 id="1-1-影响GPU性能的因素"><a href="#1-1-影响GPU性能的因素" class="headerlink" title="1.1 影响GPU性能的因素"></a>1.1 影响GPU性能的因素</h4><ol>
<li><strong>显存的类型</strong><br>显存一般分为两类：GDDR显存是韩庄在GPU附近的PCB板上；HBM2显存则被封装在GPU芯片内部，因而HBM2显存与GPU的通讯速率较高。</li>
</ol>
<p>其他的一些显存常用指标为：容量、带宽、位宽和速率。<br>位宽是指GPU一次能传递的数据宽度，位宽越大，一次性能传输的数据就越多，显卡的性能提升就越明显。<br>$$ 显存带宽(GB/s)= 显存数据频率(Gbps)\times  显存等效位宽(bit) / 8 $$</p>
<ol start="2">
<li><p><strong>CUDA core(SP) 数量</strong><br>SP(Streaming Processor)，流处理单元，也称为CUDA core，是GPU上最基本的处理单元。在GPU上运行的具体的指令和任务都是在SP上处理的。GPU进行并行计算，也就是很多个SP同时做处理。因此SP数目越多，GPU的性能就越强大。</p>
</li>
<li><p><strong>GPU之间数据传输速率</strong><br>GPU之间的互联通信有两种方案：NVLink方案和PCIe方案。使用NVLink互联的GPU通信速率一般是PCIe的近十倍。</p>
</li>
</ol>
<h4 id="1-2-显卡比较：GeForce与Tesla"><a href="#1-2-显卡比较：GeForce与Tesla" class="headerlink" title="1.2 显卡比较：GeForce与Tesla"></a>1.2 显卡比较：GeForce与Tesla</h4><ol>
<li><p><strong>ECC内存的错误检测和纠正</strong><br>GeForce系列显卡不具备错误检测和纠正的功能，但Tesla系列GPU因为GPU核心内部的寄存器、L1/L2缓存和显存都支持ECC校验功能，所以Tesla不仅能检测并纠正单比特错误也可以发现并警告双比特错误，这对保证计算结果的准确性来说非常重要.</p>
</li>
<li><p><strong>DAAA引擎</strong><br>GeForce产品_般只有单个DMA引擎，同时只能在一个方向上传输数据。如果数据正在上传到GPU，则在上传完成之前，无法返回由GPU计算的任何结果。同样，从GPU返回的结果将阻止任何需要上传到GPU的新数据Tesla GPU产品采用双DMA引擎，数据可以在CPU和GPU之间同时输入和输出，无需等待，效率更高.</p>
</li>
<li><p><strong>GPU Direct RDAAA</strong><br>NVIDIA的GPU-Direct技术可大大提高GPU之间的数据传输速度，RDMA功能则可以对多台机器之间的数据传输提供最大的性能提升。GeForce GPU只能支持单台机器内部的P2P GPU Direct,不支持跨主机的GPU-Direct RDMA。Tesla GPU 则完全支持 GPU Direct RDMA 和各种其他 GPU Direct 功能，这对GPU机器的集群部署非常有帮助。</p>
</li>
<li><p><strong>Hyper-Q的支持</strong><br>Hyper-Q代理允许多个CPU线程或进程在单个GPU上启动工作。GeForce GPU仅仅支 持CUDA Streams的Hyper-Q，也就是说GeForce只能从单独的CPU内核有效地接受并运行并行计算，但跨多台计算机运行的应用程序将无法有效地启动GPU上的工作。Tesla则具备完整的Hpyer-Q支持能力，更适合多个GPU集群的并行计算</p>
</li>
<li><p><strong>Volta架构中的 Tensor Core</strong></p>
</li>
</ol>
<h3 id="2-GPU架构-–-硬件角度"><a href="#2-GPU架构-–-硬件角度" class="headerlink" title="2 GPU架构 – 硬件角度"></a>2 GPU架构 – 硬件角度</h3><h4 id="2-1-General-Idea-GPU-v-s-CPU"><a href="#2-1-General-Idea-GPU-v-s-CPU" class="headerlink" title="2.1 General Idea : GPU v.s. CPU"></a>2.1 General Idea : GPU v.s. CPU</h4><ul>
<li>GPU与CPU结构对比：<img src="/2020/04/09/AdvancedBigdataAnalysis01/gpu_vs_cpu.jpg" alt="GPU与CPU结构对比"><table>
<thead>
<tr>
<th align="center">Pros of GPU</th>
<th></th>
<th align="center">Cons of GPU</th>
</tr>
</thead>
<tbody><tr>
<td align="center">更大的内存、带宽</td>
<td></td>
<td align="center">不能并行化的工作帮助不大</td>
</tr>
<tr>
<td align="center">更大量的执行单元</td>
<td></td>
<td align="center">不具有分支预测</td>
</tr>
<tr>
<td align="center">对比cpu，显卡的价格较为低廉</td>
<td></td>
<td align="center">GPGPU模型尚不成熟</td>
</tr>
</tbody></table>
</li>
</ul>
<h4 id="2-2-Hardware-View-of-GPU"><a href="#2-2-Hardware-View-of-GPU" class="headerlink" title="2.2 Hardware View of GPU"></a>2.2 Hardware View of GPU</h4><p><img src="/2020/04/09/AdvancedBigdataAnalysis01/gpu.jpg" alt="GPU硬件架构"><br>$$GPU = 显存(L1cache + L2cache) + 计算单元$$</p>
<h4 id="2-3-厘清SP-SM"><a href="#2-3-厘清SP-SM" class="headerlink" title="2.3 厘清SP, SM"></a>2.3 厘清SP, SM</h4><p>首先要明确：SP（streaming Process），SM（streaming multiprocessor）是硬件（GPU hardware）概念。<br>而thread，block，grid，warp是软件上的（CUDA）概念。</p>
<ul>
<li><p>术语释义</p>
<ul>
<li><p><strong>SP</strong>（Streaming Processor），流处理单元，也称为CUDA core，是GPU上最基本的处理单元。在GPU上运行的具体的指令和任务都是在SP上处理的。一个SP对应一个thread。</p>
</li>
<li><p><strong>Warp</strong>：warp是SM调度和执行的基础概念，通常一个SM中的SP(thread)会分成几个warp(也就是SP在SM中是进行分组的，物理上进行的分组)，一般每一个WARP中有32个thread。这个WARP中的32个thread(sp)是一起工作的，执行相同的指令，如果没有这么多thread需要工作，那么这个WARP中的一些thread(sp)是不工作的。</p>
<p>（每一个线程都有自己的寄存器内存和local memory，一个warp中的线程是同时执行的，也就是当进行并行计算时，线程数尽量为32的倍数。如果线程数不上32的倍数的话：假如是1，则warp会生成一个掩码，当一个指令控制器对一个warp单位的线程发送指令时，32个线程中只有一个线程在真正执行，其他31个 进程会进入静默状态。）</p>
</li>
<li><p><strong>SM</strong>（Streaming Multiprocessor）：多个SP加上其他的一些资源组成一个SM。也叫GPU大核，包括其他资源如：warp scheduler，register，shared memory等。</p>
<p>一个SM中的所有SP是先分成warp的，共享同一个memory和instruction unit（指令单元）。<br>SM可以看做GPU的心脏（对比CPU核心），register和shared memory是SM的稀缺资源。CUDA将这些资源分配给所有驻留在SM中的threads。因此，这些有限的资源就使每个SM中active warps有非常严格的限制，也就限制了并行能力。<a href="https://blog.csdn.net/junparadox/article/details/50540602" target="_blank" rel="noopener">参考链接</a>。</p>
</li>
</ul>
</li>
</ul>
<p><img src="/2020/04/09/AdvancedBigdataAnalysis01/sm.jpg" alt></p>
<h4 id="2-4-总结"><a href="#2-4-总结" class="headerlink" title="2.4 总结"></a>2.4 总结</h4><p>　　GPU中每个SM都设计成支持数以百计的线程并行执行，并且每个GPU都包含了很多的SM，所以GPU支持成百上千的线程并行执行。当一个kernel启动后，thread会被分配到这些SM中执行。大量的thread可能会被分配到不同的SM，<strong>同一个block中的threads必然在同一个SM中并行执行</strong>。每个thread拥有自己的程序计数器和状态寄存器，并且用该线程自己的数据执行指令，这就是所谓的单指令多线程结构(SIMT)。<br>　　一个SP可以执行一个thread，但是<strong>实际上并不是所有的thread能够在同一时刻执行</strong>。NVIDIA把32个threads组成一个warp。<strong>warp是调度和运行的基本单元。warp中所有threads并行的执行相同的指令。</strong>一个warp需要占用一个SM运行，多个warps需要轮流进入SM。由SM的硬件warp scheduler负责调度。目前每个warp包含32个threads（Nvidia保留修改数量的权利）。所以，一个GPU上resident thread最多只有 $SM \times warp$ 个。</p>
<h3 id="3-What-is-CUDA"><a href="#3-What-is-CUDA" class="headerlink" title="3 What is CUDA"></a>3 What is CUDA</h3><p>CUDA的全名是Computed Unified Device Architecture，是一个统一的计算框架。CUDA既不是一个软件也不是一个纯硬件，是软硬结合的计算体系。它是由NVIDIA推出的通用并行计算框架，包含CUDA指令集架构和GPU内部的并行计算引擎。从CUDA体系结构的组成来说，包含了三个部分：开发库，运行环境和驱动。</p>
<p>它诞生是为了让GPU能够有可用的编程环境，使得开发人员可以用程序控制GPU的硬件进行并行计算。</p>
<p><strong>CUDA控制的GPU程序运行过程</strong></p>
<ul>
<li><p>At the top level，我们有一个主进程，该主进程在CPU上运行并执行以下步骤：</p>
<ol>
<li>初始化GPU卡</li>
<li>在Host和Device上分配内存</li>
<li>数据从Host复制到Device内存</li>
<li>launches multiple instances of execution “kernel” on device</li>
<li>将数据从Device内存复制到Host</li>
<li>根据需要重复3-5</li>
<li>释放所有内存并终止<br><img src="/2020/04/09/AdvancedBigdataAnalysis01/cuda2.jpg" alt></li>
</ol>
</li>
<li><p>而在GPU上，有以下过程：(instance == block)</p>
<ol>
<li>execution kernel的每个instance都在SM上执行</li>
<li>如果instance数超过了SM的数量：如果有足够的寄存器和共享内存，则每个SM一次将运行多个instance。其他SM将在队列中等待并稍后执行.</li>
<li>一个instance中的所有线程都可以访问本地共享内存，但看不到其他instance在做什么（即使它们在同一SM上）。</li>
<li>无法保证instance的执行顺序。</li>
</ol>
</li>
</ul>
<h3 id="4-CUDA-Cpp-编程"><a href="#4-CUDA-Cpp-编程" class="headerlink" title="4 CUDA/Cpp 编程"></a>4 CUDA/Cpp 编程</h3><h4 id="4-1-CUDA常见术语"><a href="#4-1-CUDA常见术语" class="headerlink" title="4.1 CUDA常见术语"></a>4.1 CUDA常见术语</h4><blockquote>
<p>Compiler : NVCC<br>device = GPU<br>host = CPU<br>kernel = functions that run on the device</p>
</blockquote>
<h4 id="4-2-编程要点"><a href="#4-2-编程要点" class="headerlink" title="4.2 编程要点"></a>4.2 编程要点</h4><ul>
<li><p><strong>通过关键字定义函数，控制某个程序在CPU上跑还是在GPU上跑</strong></p>
<table>
<thead>
<tr>
<th align="center"></th>
<th></th>
<th align="center">Execute On</th>
<th align="center">Callable From</th>
</tr>
</thead>
<tbody><tr>
<td align="center">__device__ float DeviceFunc</td>
<td></td>
<td align="center">device</td>
<td align="center">device</td>
</tr>
<tr>
<td align="center">__global__ void KnernelFunc</td>
<td></td>
<td align="center">device</td>
<td align="center">host</td>
</tr>
<tr>
<td align="center">__host__ float HostFunc</td>
<td></td>
<td align="center">host</td>
<td align="center">host</td>
</tr>
</tbody></table>
</li>
<li><p><strong>CPU和GPU间的数据传输</strong></p>
<ul>
<li>GPU内存分配/回收内存的函数接口：<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMalloc</span><span class="params">(<span class="keyword">void</span> **devPtr, <span class="keyword">size_t</span>  size )</span></span>;  </span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaFree</span><span class="params">(<span class="keyword">void</span> *devPtr)</span></span>;</span><br></pre></td></tr></table></figure></li>
<li>数据传输的函数接口：<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMemcpy</span><span class="params">(<span class="keyword">void</span> *dst, <span class="keyword">const</span> <span class="keyword">void</span> *src, <span class="keyword">size_t</span> count, <span class="keyword">enum</span> cudaMemcpyKind kind)</span></span>;</span><br></pre></td></tr></table></figure>
其中 cudaMemcpyKind 有几种类型，分别是：<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">cudaMemcpyHostToDevice; <span class="comment">// CPU到GPU</span></span><br><span class="line">cudaMemcpyDeviceToHost; <span class="comment">// GPU到CPU</span></span><br><span class="line">cudaMemcpyDeviceToDevice; <span class="comment">// GPU到GPU</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>怎么用代码表示线程组织模型</strong><br>Triple angle brackets mark a call from host code to device code (also called a “kernel launch”).</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">cudaError_t kernel_routine&lt;&lt;&lt;gridDim, blockDim&gt;&gt;&gt;(args);</span><br><span class="line"><span class="comment">// 函数名称&lt;&lt;&lt;block数目，thread数目，shared memory大小&gt;&gt;&gt;(参数...);</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>gridDim</strong> is the number of instances of the kernel (the “grid” size), i.e., number of blocks</li>
<li><strong>blockDim</strong> is the number of threads within each instance (the “block” size) , i.e. , number of threads.</li>
<li>The more general form <strong>allows gridDim and blockDim to be 2D or 3D</strong> to simplify application programs</li>
</ul>
</li>
<li><p><strong>数据在GPU内存的存放</strong><br>当kernel想要取用某一块内存的数据时，需要计算数据所在的位置：<img src="/2020/04/09/AdvancedBigdataAnalysis01/index.jpg" alt></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> index = threadIdx.x + blockIdx.x * blockDim.x;</span><br></pre></td></tr></table></figure>
<ul>
<li>一个问题：当数据并不正好是blockDim.x的倍数的时候，该怎么办？</li>
<li>Ans: 在kernelFunc中多传一个参数，避免溢出<br><img src="/2020/04/09/AdvancedBigdataAnalysis01/avoid.jpg" alt><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 同时需要更新KernelFunc</span></span><br><span class="line">kernel_routine&lt;&lt;&lt; (N+M<span class="number">-1</span>)/M, M&gt;&gt;&gt;(da, db, dc, N);</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>Sample Code</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// include files</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"helper_cuda.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// random init</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">random_inits</span><span class="params">(<span class="keyword">int</span> *a, <span class="keyword">int</span> N)</span></span>&#123;</span><br><span class="line">  srand(<span class="number">2019</span>);</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;N; i++)&#123;</span><br><span class="line">    a[i] = rand()%<span class="number">100</span> + <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//kernel routine</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">add_two_vec</span><span class="params">(<span class="keyword">int</span> *a, <span class="keyword">int</span> *b, <span class="keyword">int</span> *c, <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> tid = threadIdx.x + blockDim.x*blockIdx.x;</span><br><span class="line">  <span class="keyword">if</span> (tid &lt; n) c[tid] = a[tid] + b[tid];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// main code</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">const</span> <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> *a, *b, *c;</span><br><span class="line">  <span class="keyword">int</span> *da, *db, *dc;</span><br><span class="line">  <span class="keyword">int</span> nblocks, nthreads, nsize; </span><br><span class="line"></span><br><span class="line">  <span class="comment">// initialise card</span></span><br><span class="line">  findCudaDevice(argc, argv);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// set number of blocks, and threads per block</span></span><br><span class="line">  nblocks  = <span class="number">2</span>;</span><br><span class="line">  nthreads = <span class="number">8</span>;</span><br><span class="line">  nsize    = nblocks*nthreads ;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// init vector in host device</span></span><br><span class="line">  a = (<span class="keyword">int</span> *)<span class="built_in">malloc</span>(nsize*<span class="keyword">sizeof</span>(<span class="keyword">int</span>)); random_inits(a, nsize);</span><br><span class="line">  b = (<span class="keyword">int</span> *)<span class="built_in">malloc</span>(nsize*<span class="keyword">sizeof</span>(<span class="keyword">int</span>)); random_inits(b, nsize);</span><br><span class="line">  c = (<span class="keyword">int</span> *)<span class="built_in">malloc</span>(nsize*<span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line"></span><br><span class="line">  <span class="comment">// allocate memory for array</span></span><br><span class="line">  checkCudaErrors(cudaMallocManaged(&amp;da, nsize*<span class="keyword">sizeof</span>(<span class="keyword">int</span>)));</span><br><span class="line">  checkCudaErrors(cudaMallocManaged(&amp;db, nsize*<span class="keyword">sizeof</span>(<span class="keyword">int</span>)));</span><br><span class="line">  checkCudaErrors(cudaMallocManaged(&amp;dc, nsize*<span class="keyword">sizeof</span>(<span class="keyword">int</span>)));</span><br><span class="line"></span><br><span class="line">  <span class="comment">// copy data from host to device</span></span><br><span class="line">  checkCudaErrors(cudaMemcpy(da, a, nsize*<span class="keyword">sizeof</span>(<span class="keyword">int</span>), cudaMemcpyHostToDevice));</span><br><span class="line">  checkCudaErrors(cudaMemcpy(db, b, nsize*<span class="keyword">sizeof</span>(<span class="keyword">int</span>), cudaMemcpyHostToDevice));</span><br><span class="line"></span><br><span class="line">  <span class="comment">// execute kernel</span></span><br><span class="line">  add_two_vec&lt;&lt;&lt;nblocks,nthreads&gt;&gt;&gt;(da, db, dc, nsize);</span><br><span class="line">  getLastCudaError(<span class="string">"add_teo_vec failed\n"</span>);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Get back to Host</span></span><br><span class="line">  checkCudaErrors(cudaMemcpy(c, dc, nsize*<span class="keyword">sizeof</span>(<span class="keyword">int</span>), cudaMemcpyDeviceToHost));</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Print</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;nsize; i++)&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">" a,  b, c  =  %d  %d  %d \n"</span>, a[i], b[i], c[i]);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Clean up</span></span><br><span class="line">  checkCudaErrors(cudaFree(da));</span><br><span class="line">  checkCudaErrors(cudaFree(db));</span><br><span class="line">  checkCudaErrors(cudaFree(dc));</span><br><span class="line">  <span class="built_in">free</span>(a); <span class="built_in">free</span>(b); <span class="built_in">free</span>(c);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="4-3-Others"><a href="#4-3-Others" class="headerlink" title="4.3 Others"></a>4.3 Others</h4><ul>
<li><p><strong>Coordinating Host &amp; Device</strong></p>
<ul>
<li>Kernel launches are asynchronous</li>
<li>CPU needs to synchronize before consuming the results<ul>
<li><strong>cudaMemcpy()</strong>  Blocks the CPU until the copy is complete. Copy begins when all preceding CUDA calls have completed</li>
<li><strong>cudaMemcpyAsync()</strong>   Asynchronous, does not block the CPU.</li>
<li><strong>cudaDeviceSynchronize()</strong>   Blocks the CPU until all preceding CUDA calls have completed</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Reporting Errors</strong><br>All CUDA API calls return an error code (<strong>cudaError_t</strong>). It is : Error in the API call itself <strong>OR</strong> Error in an earlier asynchronous operation (e.g. kernel).<br>To get the error code for the last error:</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaGetLastError</span><span class="params">(<span class="keyword">void</span>)</span></span>;</span><br></pre></td></tr></table></figure>
<p>To get a string to describe the error:</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">char</span> *<span class="title">cudaGetErrorString</span><span class="params">(cudaError_t)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// for example</span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%s\n"</span>, cudaGetErrorString(cudaGetLastError()));</span><br></pre></td></tr></table></figure></li>
<li><p><strong>Device Management</strong><br>Application can query and select GPUs</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaGetDeviceCount</span><span class="params">(<span class="keyword">int</span> *count)</span></span>;</span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaSetDevice</span><span class="params">(<span class="keyword">int</span> device)</span></span>;</span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaGetDevice</span><span class="params">(<span class="keyword">int</span> *device)</span></span>;</span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaGetDeviceProperties</span><span class="params">(cudaDeviceProp *prop, <span class="keyword">int</span> device)</span></span></span><br></pre></td></tr></table></figure>
<p>Multiple threads can sharea device.</p>
<p>A single thread can manage multiple devices.</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaSetDevice</span><span class="params">(i)</span></span>; <span class="comment">//to select current device</span></span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaMemcpy</span><span class="params">(...)</span></span>; <span class="comment">//for peer-to-peer copies</span></span><br></pre></td></tr></table></figure>


</li>
</ul>
<h3 id="5-CUDA内存模型-–-软件角度"><a href="#5-CUDA内存模型-–-软件角度" class="headerlink" title="5 CUDA内存模型 – 软件角度"></a>5 CUDA内存模型 – 软件角度</h3><p><img src="/2020/04/09/AdvancedBigdataAnalysis01/cuda.jpg" alt></p>
<ul>
<li><strong>thread</strong>：一个CUDA的并行程序会被以许多个threads来执行。每个 thread 都有自己的一份 register 和 local memory 的空间。</li>
<li><strong>block</strong>：一组 thread 构成一个 block，这些 thread 共享一份 shared memory。</li>
<li><strong>grid</strong>：多个blocks会构成一个grid。不同的 grid 有各自的 global memory、constant memory 和 texture memory。<strong>同一个grid内所有的thread(包括不同block的thread)都可以共享这些global memory、constant memory、和 texture memory。</strong></li>
</ul>
<p>线程访问这几类存储器的速度是：register &gt; local memory &gt;shared memory &gt; global memory。</p>
<p>每一个时钟周期内，warp（一个block里面一起运行的thread）包含的thread数量是有限的，现在的规定是32个。其中各个线程对应的数据资源不同(指令相同但是数据不同）。一个block中含有16个warp。所以一个block中最多含有512个线程。</p>
<h3 id="6-内存共享与同步-an-example"><a href="#6-内存共享与同步-an-example" class="headerlink" title="6 内存共享与同步: an example"></a>6 内存共享与同步: an example</h3><p><strong>__shared__</strong> declares data that is available for all threads within a certain block. (not visible to threads in other blocks)</p>
<p>Use <strong>__syncthreads()</strong> as a barrier to prevent data hazard.</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">stencil_1d</span><span class="params">(<span class="keyword">int</span> *in, <span class="keyword">int</span> *out)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// declare a shared array, availabel for all threads in this block</span></span><br><span class="line">    __shared__ <span class="keyword">int</span> temp[BLOCK_SIZE + <span class="number">2</span> * RADIUS];</span><br><span class="line">    <span class="keyword">int</span> gindex = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="keyword">int</span> lindex = threadIdx.x + radius;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Read input elements into shared memory</span></span><br><span class="line">    temp[lindex] = in[gindex]; </span><br><span class="line">    <span class="keyword">if</span> (threadIdx.x &lt; RADIUS) &#123;</span><br><span class="line">        temp[lindex – RADIUS] = in[gindex – RADIUS];</span><br><span class="line">        temp[lindex + BLOCK_SIZE] = in[gindex + BLOCK_SIZE]; </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Because we have no idea which thread runs first, we need to</span></span><br><span class="line">    <span class="comment">// Synchronize (ensure all the data is available)</span></span><br><span class="line">    __syncthreads( );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Apply the stencil (i.e. 1-D conv)</span></span><br><span class="line">    <span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> offset = -RADIUS ; offset &lt;= RADIUS ; offset++)</span><br><span class="line">        result += temp[lindex + offset];</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Store the result</span></span><br><span class="line">    out[gindex] = result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>CUDA</tag>
        <tag>GPU</tag>
        <tag>Cpp</tag>
      </tags>
  </entry>
  <entry>
    <title>MapReduce编程实践1</title>
    <url>/2020/04/08/Large-scaleDistributedSystem01/</url>
    <content><![CDATA[<h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><p>MapReduce是一种新的分布式并行计算框架，能够在HDFS和Yarn的支持下完成海量大数据计算与处理的任务。最早起源于Google的GFS系统，是Google为了处理PB级别的网页数据而设计的。<br>MapReduce的优势在于可以对海量数据进行并行计算，并且能够高效的部署在廉价的计算机集群上，因而受到广泛使用。它的基本思想是分而治之。</p>
<p>MapReduce借鉴了函数式程序设计语言Lisp中的思想，定义了如下的Map和Reduce两个抽象的编程接口，由用户去编程实现:</p>
<ul>
<li>Map: (k1; v1) -&gt; [(k2; v2)]</li>
<li>Reduce: : (k2; [v2]) -&gt; [(k3; v3)]</li>
</ul>
<p>各个map函数对所划分的数据并行处理，从不同的输入数据产生不同的中间结果输出。<br>各个reduce也各自并行计算，各自负责处理不同的中间结果数据集合。</p>
<a id="more"></a>

<h3 id="本次任务"><a href="#本次任务" class="headerlink" title="本次任务"></a>本次任务</h3><p>编写MR程序（java或python）对sample.txt中有关文件信息进行处理，任务如下：<br>&nbsp;&nbsp;&nbsp;&nbsp;1.统计其中各类文件的数量（按文件名后缀区分类型）<br>&nbsp;&nbsp;&nbsp;&nbsp;2.按文件的字节数大小降序排序输出文件名</p>
<p>sample.txt 包含85行建筑设计中的各类文件信息，数据片段和格式说明如下:</p>
<table>
    <tr>
        <th rowspan="3">sample.txt</th>
        <th>日期</th>
        <th>时间</th>
        <th>字节数</th>
        <th>文件名（后缀为dwg）</th>
    </tr>
    <tr>
        <td>2014/10/21</td>
        <td>07:30</td>
        <td>3,395,145</td>
        <td>02-01一层平面图.dwg</td>
    </tr>
    <tr>
        <td>2014/10/21</td>
        <td>07:29</td>
        <td>924,099</td>
        <td>02-02二层平面图.dwg</td>
</tr></table>

<h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><p>环境配置</p>
<ul>
<li>阿里云ECS Ubuntu 16.04</li>
<li>java: open jdk-7</li>
<li>hadoop: 2.9.2</li>
<li>python: 3.5<ul>
<li>mrjob 0.7.1</li>
</ul>
</li>
</ul>
<h4 id="统计各类文件的数量"><a href="#统计各类文件的数量" class="headerlink" title="统计各类文件的数量"></a>统计各类文件的数量</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mrjob.job <span class="keyword">import</span> MRJob</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">pattern1 = <span class="string">r"[0-9]*,*[0-9]+,[0-9]&#123;3&#125;"</span> <span class="comment"># number of bytes</span></span><br><span class="line">pattern2 = <span class="string">r"\.[a-z]+"</span> <span class="comment"># file type</span></span><br><span class="line">pattern3 = <span class="string">r" [^ ]+\.[a-z]+$"</span> <span class="comment"># file name</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">str2int</span><span class="params">(input)</span>:</span></span><br><span class="line">    a = <span class="string">""</span>.join(input.split(<span class="string">','</span>))</span><br><span class="line">    <span class="keyword">return</span> int(a)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CountWorker</span><span class="params">(MRJob)</span>:</span></span><br><span class="line">    <span class="string">""" Count the number of each type of files. """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mapper</span><span class="params">(self, file_name, line)</span>:</span></span><br><span class="line">        <span class="string">""" Input Context: Date, Time, #Bytes, Name """</span></span><br><span class="line">        file_type = re.search(pattern2, line).group()[<span class="number">1</span>:]</span><br><span class="line">        <span class="keyword">yield</span> file_type, <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reducer</span><span class="params">(self, key, values)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> key, sum(values)</span><br></pre></td></tr></table></figure>

<h4 id="按文件字节数降序输出文件名"><a href="#按文件字节数降序输出文件名" class="headerlink" title="按文件字节数降序输出文件名"></a>按文件字节数降序输出文件名</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RankWorker</span><span class="params">(MRJob)</span>:</span></span><br><span class="line">    <span class="string">""" Ranking in the descending order of #Bytes. """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mapper</span><span class="params">(self, file_name, line)</span>:</span></span><br><span class="line">        <span class="string">""" Input Context: Date, Time, #Bytes, Name """</span></span><br><span class="line">        num_bytes = str2int(re.search(pattern1, line).group())</span><br><span class="line">        Name = re.search(pattern3, line).group()[<span class="number">1</span>:]</span><br><span class="line">        <span class="keyword">yield</span> <span class="number">1</span>, (num_bytes, Name)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reducer</span><span class="params">(self, key, values)</span>:</span></span><br><span class="line">        values = sorted(values)</span><br><span class="line">        <span class="keyword">for</span> num_bytes, Name <span class="keyword">in</span> values[::<span class="number">-1</span>]:</span><br><span class="line">            <span class="keyword">yield</span> num_bytes, Name</span><br></pre></td></tr></table></figure>

<h4 id="输出中文乱码问题"><a href="#输出中文乱码问题" class="headerlink" title="输出中文乱码问题"></a>输出中文乱码问题</h4><p>注意，这里由于一开始的时候没有对sample.txt文件进行转码，而且由于MRjob的输出设定，返回的字符串并不是utf-8编码的。因此需要手动进行一下解码（可能需要多次尝试）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deal_with_output</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    lines = []</span><br><span class="line">    <span class="keyword">with</span> open(file_path, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">            line = f.readline()</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> line: <span class="keyword">break</span></span><br><span class="line">            tmp = line.strip().split(<span class="string">"\t"</span>)</span><br><span class="line">            </span><br><span class="line">            num_bytes = int(tmp[<span class="number">0</span>])</span><br><span class="line">            name = <span class="string">""</span>.join([tmp[<span class="number">1</span>][<span class="number">1</span>:<span class="number">-1</span>]])</span><br><span class="line">            </span><br><span class="line">            print(num_bytes, name.encode(<span class="string">"iso-8859-1"</span>).decode(<span class="string">"gb18030"</span>))</span><br><span class="line">            <span class="comment"># 这里还是有bug，建议根据情况自己试一下</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/04/07/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<a id="more"></a>

<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>MapReduce编程实践2：倒排文档索引</title>
    <url>/2020/04/08/Large-scaleDistributedSystem02/</url>
    <content><![CDATA[<h3 id="1-具有payload的倒排文档索引"><a href="#1-具有payload的倒排文档索引" class="headerlink" title="1.具有payload的倒排文档索引"></a>1.具有payload的倒排文档索引</h3><p>Inverted Index(倒排索引)是几乎所有全文检索搜索引擎都在使用的一种数据索引技术。<br>采用倒排索引，对于给定的查询词(term)，能快速或取包含有该term的文档列表(the list of documents)。</p>
<a id="more"></a>

<p>简单的倒排文档索引示意图:</p>
<p><img src="/2020/04/08/Large-scaleDistributedSystem02/invdoc1.png" alt="简单的倒排文档索引示意图"></p>
<p>如果考虑单词在每个文档中出现的词频、位置、对应Web文档的URL等诸多属性，则简单的倒排算法就需要进一步的改进。我们把这些词频、位置等诸多属性称为有效负载（Payload）。</p>
<img src="/2020/04/08/Large-scaleDistributedSystem02/invdoc2.png" class>
<p>一个倒排索引由大量的postings list构成，每个posting list与一个term相关联。一个posting 包含一个document id和一个payload。payload上载有term在document中出现情况相关的信息（例如：出现位置，词频等等）。</p>
<p>前述提到，MapReduce的核心是Map和Reduce两个函数。Map用于生成会被Reduce使用的中间结果。由于MR系统是运行在分布式集群上的，所以中间结果会被Partition和Combine(可选的)。在Parition过程中，系统自动按照map的输出键进行排序，因此，进入Reduce节点的(key, {value})对将保证是按照key进行排序的，而{value}则不保证是排好序的。</p>
<p>那如果想要对value进行排序，应该怎么办呢？</p>
<ul>
<li>一种办法是在Reduce任务中，对{value}表中的各个value进行排序。但当{value}列表数据量巨大、无法在本地内存中进行排序时，将出现问题。</li>
<li>更好的办法是利用MapReduce过程中的Partition过程会对Map输出key进行排序的效应，在分发过程中就完成对value的排序。想要做到这一点，就需要将value中需要排序的部分加入到key中，使key成为复合键。 </li>
</ul>
<p>举例来说，本来Map任务输出的键值对是（word, (doc_titile, word_frequency)）。现在为了实现对word_frequency的排序，我们将Map的输出重置为（(word, word_frequency), doc_titile）。</p>
<p>但这样会带来一个问题：我们仍然想要将同一个word对应的key-value pair放到同一个reduce任务下，但是现在的key是一个复合键。为此，需要实现一个新的Partitioner：从（word, word_frequency）中取出word，以word作为key进行分区。</p>
<p>了解到这些以后，就可以进行MR倒排文档生成了。</p>
<h3 id="2-本次任务"><a href="#2-本次任务" class="headerlink" title="2.本次任务"></a>2.本次任务</h3><ul>
<li>倒排文档生成函数：带有payload的倒排文档索引</li>
<li>文档查询函数：以contenttile为文本名。实现根据查询关键词，返回相关文档名（序列）</li>
<li>提交查询返回结果的截屏</li>
</ul>
<p>数据样本下载地址：<a href="https://pan.baidu.com/s/1UZHOMw_uhLMBCLX9uBBAQA" target="_blank" rel="noopener">密码 : rdx8</a></p>
<h3 id="3-代码示例"><a href="#3-代码示例" class="headerlink" title="3.代码示例"></a>3.代码示例</h3><p>环境配置</p>
<ul>
<li>阿里云ECS Ubuntu 16.04</li>
<li>java: open jdk-7</li>
<li>hadoop: 2.9.2</li>
<li>python: 3.5</li>
</ul>
<h4 id="3-1-解决中文乱码问题"><a href="#3-1-解决中文乱码问题" class="headerlink" title="3.1 解决中文乱码问题"></a>3.1 解决中文乱码问题</h4><p>由于下载的文件不是utf-8编码，而是gbk编码的。为了后续编程方便和一致性(jieba的官方文档说如果输入的字符串是gbk编码可能会出现未知错误)，建议将文件先转码为utf-8编码。具体操作是：首先在MacOS文本编辑器的偏好设置里，将文件的打开编码设置为gbk，保存编码设置为utf-8，然后打开下载的文件，另存为…即可。然后要记得把文本编辑器的偏好设置改回来（即打开/关闭编码都最好是utf-8）。</p>
<p>然后，将下载的文件scp到已经配好环境的ECS服务器上。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp local_file_path username@server:host_file_path</span><br></pre></td></tr></table></figure>
<p>这里建议不管是不是使用云服务器，都检查一下当前使用终端的字符集是否支持中文。如果不支持，在服务器上还是会出现中文乱码问题，可以参照这个链接：<a href="https://blog.csdn.net/weixin_34236497/article/details/94046218" target="_blank" rel="noopener">ubuntu16.04解决文件中文乱码问题</a>。</p>
<h4 id="3-2-mapper-py"><a href="#3-2-mapper-py" class="headerlink" title="3.2 mapper.py"></a>3.2 mapper.py</h4><p>与上一篇mapreduce实践不同，这次因为要实现payload，所以选用了hadoop streaming来编程。相比mrjob，hadoop streaming更为灵活，用户能够自定义更多特性，而且输入输出都是字符串的特性也比较简单好操作。具体的一些工作原理可以参考官方文档：<a href="http://hadoop.apache.org/docs/r1.0.4/cn/streaming.html" target="_blank" rel="noopener">Hadoop Streaming</a>。</p>
<p>这里简单介绍一下，hadoop streaming支持多种语言进行编程，包括java 和python。它只规定了mapper和reducer都必须是可执行文件。这次实践我们采用hadoop streaming + python。</p>
<p>mapper.py从stdin中读入文件，它要做的事是从输入的字符串中：</p>
<ol>
<li>识别出文档内容和文档名称</li>
<li>对文档内容进行分词（需要安装python第三方库jieba）</li>
<li>返回(单词，所在文档，词频)这样的key-value pair</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">pattern1 = <span class="string">r"&lt;[a-zA-Z]*&gt;"</span>  <span class="comment"># &lt;&gt;</span></span><br><span class="line">pattern2 = <span class="string">r"&gt;[^a-z]*"</span>  <span class="comment"># &gt;&lt;</span></span><br><span class="line">pattern3 = <span class="string">r"\".+?\""</span></span><br><span class="line"></span><br><span class="line">contents = []</span><br><span class="line">titles = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">    result = re.search(pattern1, line)</span><br><span class="line">    <span class="keyword">if</span> result <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        typo = result.group()[<span class="number">1</span>:<span class="number">-1</span>]</span><br><span class="line">        _text = re.search(pattern2, line).group()[<span class="number">1</span>:<span class="number">-2</span>] <span class="comment"># omit "\n" and "&gt;"</span></span><br><span class="line">        <span class="keyword">if</span> typo == <span class="string">"content"</span>:</span><br><span class="line">            contents.append(_text)</span><br><span class="line">        <span class="keyword">elif</span> typo == <span class="string">"contenttitle"</span>:</span><br><span class="line">            _text = _text[:<span class="number">-1</span>] <span class="keyword">if</span> (_text != <span class="string">""</span> <span class="keyword">and</span> _text[<span class="number">-1</span>] == <span class="string">"（"</span>) <span class="keyword">else</span> _text</span><br><span class="line">            titles.append(_text)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(titles)):</span><br><span class="line">    title, doc = titles[i], contents[i]</span><br><span class="line">    seg_list = list(jieba.cut(doc, cut_all=<span class="literal">True</span>)) <span class="comment"># precise mode</span></span><br><span class="line">    counter = Counter(seg_list)</span><br><span class="line">    <span class="keyword">for</span> pos, word <span class="keyword">in</span> enumerate(set(seg_list)):</span><br><span class="line">        print(<span class="string">"&#123;&#125;@&#123;&#125;@&#123;&#125;"</span>.format(word, counter[word], title))</span><br></pre></td></tr></table></figure>

<h4 id="3-3-reducer-py"><a href="#3-3-reducer-py" class="headerlink" title="3.3 reducer.py"></a>3.3 reducer.py</h4><p>reducer要做的事情，是从stdin中读入mapper的输出，按照（单词，所有包含该单词的文档）形式组织好数据并输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">out_dict = defaultdict(list)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">    key, value = line.split(<span class="string">"\t"</span>)</span><br><span class="line">    word, count = key.split(<span class="string">"@"</span>)</span><br><span class="line">    out_dict[word].append(value[:<span class="number">-1</span>]+<span class="string">"@"</span>+count)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word, value_list <span class="keyword">in</span> out_dict.items():</span><br><span class="line">    titles = <span class="string">"&lt;SEP&gt;"</span>.join(value_list)</span><br><span class="line">    print(<span class="string">"&#123;&#125;\t&#123;&#125;"</span>.format(word, titles))</span><br></pre></td></tr></table></figure>

<h4 id="3-4-run-sh"><a href="#3-4-run-sh" class="headerlink" title="3.4 run.sh"></a>3.4 run.sh</h4><p>在前文中我们有提到，为了将同一个word对应的key-value pair放到同一个reduce任务下，需要实现一个新的Partitioner：从（word, word_frequency）中取出word，以word作为key进行分区。在Hadoop Streaming中这个具体应该怎么实现呢？</p>
<p>很简单。Hadoop有一个工具类org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner， 它在应用程序中很有用。Map/reduce框架用这个类切分map的输出， 切分是基于key值的前缀，而不是整个key。举例来说：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="variable">$HADOOP_HOME</span>/bin/hadoop  jar <span class="variable">$HADOOP_HOME</span>/hadoop-streaming.jar \</span><br><span class="line">    -input myInputDirs \</span><br><span class="line">    -output myOutputDir \</span><br><span class="line">    -mapper org.apache.hadoop.mapred.lib.IdentityMapper \</span><br><span class="line">    -reducer org.apache.hadoop.mapred.lib.IdentityReducer \</span><br><span class="line">    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \</span><br><span class="line">    -jobconf stream.map.output.field.separator=. \</span><br><span class="line">    -jobconf stream.num.map.output.key.fields=4 \</span><br><span class="line">    -jobconf map.output.key.field.separator=. \</span><br><span class="line">    -jobconf num.key.fields.for.partition=2 \</span><br><span class="line">    -jobconf mapred.reduce.tasks=12</span><br></pre></td></tr></table></figure>
<p>其中，Streaming使用-jobconf stream.map.output.field.separator=. 和-jobconf stream.num.map.output.key.fields=4这两个变量来得到mapper的key/value对。</p>
<p>上面的Map/Reduce 作业中map输出的key一般是由“.”分割成的四块。但是因为使用了 -jobconf num.key.fields.for.partition=2 选项，所以Map/Reduce框架使用key的前两块来切分map的输出。其中， -jobconf map.output.key.field.separator=. 指定了这次切分使用的key的分隔符。这样可以保证在所有key/value对中， key值前两个块值相同的所有key被分到一组，分配给一个reducer。</p>
<p>这种高效的方法等价于指定前两块作为主键，后两块作为副键。 主键用于切分块，主键和副键的组合用于排序。</p>
<p>因此，我们只需要在执行程序的时候指明所用的partitioner和分区，就可以实现“按照一部分key的值进行排序”的功能。这里，为了避免每次启动hadoop的时候都要反复手动输入shell脚本，我写了一个.sh文件来保存会在终端进行的输入。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rm ./part-00000 <span class="comment"># remove loacal file</span></span><br><span class="line">hdfs dfs -rm -r YOUR_PATH/output/ <span class="comment"># remove output dir in hdfs, otherwise it will cause en error</span></span><br><span class="line"></span><br><span class="line">hadoop jar <span class="variable">$HADOOP_HOME</span>/share/hadoop/tools/lib/hadoop-streaming-2.9.2.jar \</span><br><span class="line">-input YOUR_PATH/inputs/news_tensite_xml.smarty.dat \</span><br><span class="line">-output YOUR_PATH/output/ \</span><br><span class="line">-mapper <span class="string">"python3 mapper.py"</span> \</span><br><span class="line">-reducer <span class="string">"python3 reducer.py"</span> \</span><br><span class="line">-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \</span><br><span class="line">-jobconf stream.map.output.field.separator=@ \</span><br><span class="line">-jobconf stream.num.map.output.key.fields=2 \</span><br><span class="line">-jobconf map.output.key.field.separator=@ \</span><br><span class="line">-jobconf num.key.fields.for.partition=1 \</span><br><span class="line"></span><br><span class="line">hdfs dfs -get bigdata/output/part-00000 ./ <span class="comment"># copy file from hdfs to local</span></span><br><span class="line">head -100 part-00000 <span class="comment"># look at content</span></span><br></pre></td></tr></table></figure>

<h4 id="3-5-query-py"><a href="#3-5-query-py" class="headerlink" title="3.5 query.py"></a>3.5 query.py</h4><p>文档查询函数：以contenttile为文本名。实现根据查询关键词，返回相关文档名（序列）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">query_dict = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"./part-00000"</span>, <span class="string">"r"</span>, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    lines = f.readlines()</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">        word, rest = line.split(<span class="string">"\t"</span>)</span><br><span class="line">        titles = rest.split(<span class="string">"&lt;SEP&gt;"</span>)</span><br><span class="line">        query_dict[word] = titles</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">query</span><span class="params">(word)</span>:</span></span><br><span class="line">    print(word)</span><br><span class="line">    leng = len(word)</span><br><span class="line">    <span class="keyword">for</span> title <span class="keyword">in</span> query_dict.get(word, [<span class="string">""</span>]):</span><br><span class="line">        print(<span class="string">" "</span> * leng + <span class="string">"\t"</span>, title)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    query(<span class="string">"经济"</span>)</span><br><span class="line">    query(<span class="string">"儿童"</span>)</span><br></pre></td></tr></table></figure>
<h4 id="3-5-Outcome"><a href="#3-5-Outcome" class="headerlink" title="3.5 Outcome"></a>3.5 Outcome</h4><p>query结果：<br><img src="/2020/04/08/Large-scaleDistributedSystem02/invdoc3.png" alt="query_outcome"></p>
<p>这个query结果还是不完善的，可以看到输出的文档名并没有完全按照词频排序（局部有序）。当然，也可以让输出文档名按照tf-idf排序。具体的实现这里就不再赘述。</p>
<h3 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h3><p>在本次实践中，学习了如何避免linux出现中文乱码问题。学会了基础的python和hadoop streaming交互，了解了hadoop streaming的基本使用。进一步加深了对MapReduce框架的理解。</p>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
        <tag>Python</tag>
      </tags>
  </entry>
</search>
